{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c4240-4889-4de6-bc65-dc02ac5aef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "PPCA v1.0.5\n",
    "\n",
    "APPENDIX A1 : SLOPE CALCULATION AND CLUSTERING FOR CATCHMENT AREAS\n",
    "\n",
    "Author: Perez, Joan\n",
    "\n",
    "This script computes population estimates and slopes across four catchment distances using log-transformed population values. It clusters \n",
    "points by combining hierarchical clustering (for initial cluster centers) with k-means clustering, using the three slopes and the \n",
    "log-transformed population value at the second distance. Outputs include clustering results and visualizations of population curves \n",
    "and silhouette scores.\n",
    "\n",
    "Full description, metadata, and output descriptions available here:\n",
    "https://github.com/perezjoan/Population-Potential-on-Catchment-Area---PPCA-Worldwide/tree/main\n",
    "\n",
    "Acknowledgement\n",
    "This resource was produced within the emc2 project, which is funded by ANR (France), FFG (Austria), MUR (Italy) and Vinnova (Sweden) under\n",
    "the Driving Urban Transition Partnership, which has been co-funded by the European Commission.\n",
    "\n",
    "License: Attribution-ShareAlike 4.0 International - CC-BY-SA-4.0 license\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071fdc3c-96bf-4441-8b06-5c5dea1a1556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------#\n",
    "# 0.1 : Box to fil with informations                                              #\n",
    "# Name of the case study - Expected format : 'Nice'                               #\n",
    "Name = ''                                                                         #\n",
    "                                                                                  #\n",
    "# Define the distances vector                                                     #\n",
    "import numpy as np                                                                #\n",
    "distances = np.array([160, 400, 800, 1200])   # <- need 4 distances here          #\n",
    "#---------------------------------------------------------------------------------#\n",
    "# 0.2 : Libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sys\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import gc\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 0.3 Data preparation\n",
    "gpkg = f'PPCA_4-1_{Name}_POP_CAT.gpkg'\n",
    "nodes = gpd.read_file(gpkg, layer='points_catchment_stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c876810-7c32-4393-b806-4043e3546344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SLOPES EXTRACTION PER ROW\n",
    "\n",
    "# Dynamically create column names based on the distances vector\n",
    "columns_to_keep = [f'cc_Pop_estimation_sum_{dist}_nw' for dist in distances]\n",
    "\n",
    "# Copy nodes DataFrame and create a new GeoDataFrame with only selected columns and geometry\n",
    "nodes_selected = gpd.GeoDataFrame(nodes[columns_to_keep], geometry=nodes.geometry)\n",
    "\n",
    "# Add 1 to each of the specified columns in nodes before log transformation (to avoid 0)\n",
    "nodes_selected[columns_to_keep] = nodes_selected[columns_to_keep] + 1\n",
    "\n",
    "# Create log-transformed columns in nodes_selected\n",
    "for col in columns_to_keep:\n",
    "    nodes_selected[f'log_{col}'] = np.log(nodes_selected[col])\n",
    "\n",
    "# Log-transformed distances\n",
    "x_points = np.log(distances)\n",
    "\n",
    "# Function to calculate r2, beta, alpha (intercept), y estimates, and slopes for each row\n",
    "def calculate_slopes(row):\n",
    "    # y-values: log-transformed population estimates (use dynamically created column names)\n",
    "    y_values = [row[f'log_cc_Pop_estimation_sum_{dist}_nw'] for dist in distances]\n",
    "    \n",
    "    # Reshape x_points\n",
    "    X = x_points.reshape(-1, 1)\n",
    "    \n",
    "    # Calculate slopes between each consecutive pair of y estimates\n",
    "    slope_1_2 = (y_values[1] - y_values[0]) / (x_points[1] - x_points[0])\n",
    "    slope_2_3 = (y_values[2] - y_values[1]) / (x_points[2] - x_points[1])\n",
    "    slope_3_4 = (y_values[3] - y_values[2]) / (x_points[3] - x_points[2])\n",
    "    \n",
    "    # Return results as a Series\n",
    "    return pd.Series({\n",
    "        'slope_1_2': slope_1_2,\n",
    "        'slope_2_3': slope_2_3,\n",
    "        'slope_3_4': slope_3_4\n",
    "    })\n",
    "\n",
    "# Split nodes_selected into batches of 1000 for progress tracking\n",
    "batch_size = 1000\n",
    "for start in range(0, len(nodes_selected), batch_size):\n",
    "    end = min(start + batch_size, len(nodes_selected))\n",
    "    nodes_selected.loc[start:end, ['slope_1_2', 'slope_2_3', 'slope_3_4']] = nodes_selected.iloc[start:end].apply(calculate_slopes, \n",
    "                                                                                                                 axis=1)\n",
    "    \n",
    "    # Progress display\n",
    "    progress = (end / len(nodes_selected)) * 100\n",
    "    sys.stdout.write(f\"\\rProcessed {progress:.2f}% of rows\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "sys.stdout.write(\"\\rProcessed 100.00% of rows\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33178171-9968-41d6-94a3-338941df3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. HIERARCHICAL CLUSTERING ON SAMPLE (TO DEFINE CLUSTER CENTERS START)\n",
    "\n",
    "# Sampling 50,000 nodes for clustering\n",
    "sample_size = 50000  # Adjust this as needed\n",
    "sampled_nodes = nodes_selected.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Define the clustering features (intercept, slopes, and log-transformed y-values)\n",
    "features_sampled = sampled_nodes[['slope_1_2', 'slope_2_3', 'slope_3_4', \n",
    "                                  f'log_cc_Pop_estimation_sum_{distances[1]}_nw']]\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features_sampled = scaler.fit_transform(features_sampled)\n",
    "\n",
    "# Perform hierarchical clustering directly with the scaled features\n",
    "linkage_matrix_sampled = linkage(scaled_features_sampled, method='ward')\n",
    "\n",
    "# Calculate the merge distances for clusters between 5 and 20\n",
    "distances2 = linkage_matrix_sampled[:, 2]  # The third column contains the linkage distances\n",
    "desired_clusters = np.arange(5, 21)  # Cluster range from 5 to 20\n",
    "merge_distances = [distances2[len(distances2) - cluster_count] for cluster_count in desired_clusters]\n",
    "\n",
    "# Calculate the absolute differences between successive merge distances\n",
    "gap_differences = np.diff(merge_distances)\n",
    "\n",
    "# Identify the indices of the top 3 largest gaps using absolute values\n",
    "largest_gaps_indices = np.argsort(np.abs(gap_differences))[-3:][::-1]  # Top 3 largest gaps in descending order\n",
    "\n",
    "# Get the merge distances corresponding to the largest gaps\n",
    "top_merge_distances = [merge_distances[i] for i in largest_gaps_indices]\n",
    "\n",
    "# Validate the cluster counts for each merge distance\n",
    "cluster_counts = [\n",
    "    len(np.unique(fcluster(linkage_matrix_sampled, t=merge_distance, criterion='distance')))\n",
    "    for merge_distance in top_merge_distances\n",
    "]\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linkage_matrix_sampled, truncate_mode='level', p=15)\n",
    "\n",
    "# Add horizontal lines for the top merge distances\n",
    "for merge_distance in top_merge_distances:\n",
    "    plt.axhline(y=merge_distance, color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "# Title and labels\n",
    "plt.title(\"Dendrogram with Top 3 Largest Gaps\")\n",
    "plt.xlabel(\"\")  # Remove x-axis label\n",
    "plt.xticks([])  # Remove all x-axis ticks and labels\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "\n",
    "# Get the number of clusters for each top merge distance\n",
    "cluster_counts = [\n",
    "    len(np.unique(fcluster(linkage_matrix_sampled, t=merge_distance, criterion='distance'))) + 1\n",
    "    for merge_distance in top_merge_distances\n",
    "]\n",
    "\n",
    "# Print the number of clusters for each merge distance\n",
    "print(\"Number of clusters corresponding to each threshold (top lines):\", cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388b175-f055-4f95-a1c0-907ae0ef811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. KMEANS CLUSTERING WITH DEFINED STARTING CENTERS\n",
    "\n",
    "# Function to get initial medoids based on hierarchical clustering\n",
    "def get_sampled_initial_centers(features_sampled, dist_matrix, n_clusters):\n",
    "    cluster_labels_sampled = fcluster(linkage_matrix_sampled, n_clusters, criterion='maxclust')\n",
    "    initial_centers = []\n",
    "    for cluster_id in range(1, n_clusters + 1):\n",
    "        cluster_indices = np.where(cluster_labels_sampled == cluster_id)[0]\n",
    "        cluster_center = features_sampled.iloc[cluster_indices].mean(axis=0).to_numpy()\n",
    "        initial_centers.append(cluster_center)\n",
    "    return np.array(initial_centers)\n",
    "\n",
    "# Ensure full_features uses the entire nodes_selected dataset, not the sampled nodes\n",
    "full_features = nodes_selected[['slope_1_2', 'slope_2_3', 'slope_3_4', \n",
    "                                f'log_cc_Pop_estimation_sum_{distances[1]}_nw']]\n",
    "scaled_full_features = scaler.transform(full_features)  # Scale the full dataset\n",
    "\n",
    "# Compute pairwise distance matrix for sampled features\n",
    "sampled_dist_matrix = squareform(pdist(scaled_features_sampled, metric='euclidean'))\n",
    "\n",
    "# Perform K-Means clustering with initialized centers\n",
    "def perform_kmeans_with_sampled_initialization(full_features, sampled_features, dist_matrix, n_clusters):\n",
    "    # Get initial cluster centers from sampled data\n",
    "    initial_centers = get_sampled_initial_centers(pd.DataFrame(sampled_features), dist_matrix, n_clusters)\n",
    "    \n",
    "    # Run K-Means on the full dataset using the initial centers\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init=initial_centers, n_init=1, random_state=42)\n",
    "    kmeans.fit(full_features)  # Fit K-Means on the full dataset\n",
    "    return kmeans.labels_  # Return labels for the full dataset\n",
    "\n",
    "# Perform clustering for different cluster sizes\n",
    "for n_clusters in np.arange(5, 20):\n",
    "    print(f\"Clustering for {n_clusters} clusters started...\")\n",
    "    labels = perform_kmeans_with_sampled_initialization(\n",
    "        scaled_full_features,  # Full dataset\n",
    "        scaled_features_sampled,  # Sampled dataset\n",
    "        sampled_dist_matrix,  # Distance matrix of the sampled dataset\n",
    "        n_clusters\n",
    "    )\n",
    "    # Add clustering results to the full GeoDataFrame\n",
    "    nodes_selected[f'hckmeans_{n_clusters}_clusters'] = labels\n",
    "    print(f\"Clustering for {n_clusters} clusters completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505283f-75eb-489b-a25f-478230f8915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of cluster counts\n",
    "n_clusters_vector = np.arange(5, 20)  # Cluster solutions from 5 to 19\n",
    "\n",
    "# Initialize a dictionary to store silhouette scores\n",
    "silhouette_scores = {}\n",
    "\n",
    "# Sample the dataset to reduce computation time\n",
    "sampled_data = nodes_selected.sample(n=10000, random_state=42)  # Adjust sample size as needed\n",
    "\n",
    "# Compute silhouette scores for each clustering solution\n",
    "for n_clusters in n_clusters_vector:\n",
    "    # Retrieve cluster labels for the sampled data\n",
    "    labels = sampled_data[f'hckmeans_{n_clusters}_clusters']\n",
    "    \n",
    "    # Compute the silhouette score using relevant feature columns\n",
    "    silhouette_scores[n_clusters] = silhouette_score(\n",
    "        sampled_data[['slope_1_2', 'slope_2_3', 'slope_3_4']].values,  # Feature matrix\n",
    "        labels  # Cluster labels\n",
    "    )\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    list(silhouette_scores.keys()), \n",
    "    list(silhouette_scores.values()), \n",
    "    marker='o', linestyle='-', color='b'\n",
    ")\n",
    "plt.title(\"Silhouette Scores for Different Cluster Counts\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.xticks(n_clusters_vector)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbf146-7166-4d8a-abdf-b08e61610b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features to be used in the line plot\n",
    "features = [f'log_cc_Pop_estimation_sum_{distances[0]}_nw', f'log_cc_Pop_estimation_sum_{distances[1]}_nw',\n",
    "           f'log_cc_Pop_estimation_sum_{distances[2]}_nw', f'log_cc_Pop_estimation_sum_{distances[3]}_nw']\n",
    "num_vars = len(features)\n",
    "\n",
    "# Create a scaled copy of the dataset for the line plots\n",
    "data_selected = nodes_selected.copy()\n",
    "\n",
    "# Define the function to plot feature curves for multiple cluster solutions\n",
    "def plot_feature_curves_multiple(data, title_prefix, n_clusters_vector):\n",
    "    # Custom y-ticks and labels\n",
    "    y_ticks = np.arange(0, 10)  # Tick positions (0 to 9)\n",
    "    y_labels = ['0', '1 (1)', '2 (5)', '3 (20)', '4 (50)', '5 (150)', '6 (400)', '7 (1200)', '8 (3000)', '9 (8000)']\n",
    "\n",
    "    for n_clusters in n_clusters_vector:\n",
    "        cluster_column = f'hckmeans_{n_clusters}_clusters'\n",
    "\n",
    "        # Compute the average of each feature per cluster in the specified column\n",
    "        cluster_means = data.groupby(cluster_column)[features].mean()\n",
    "\n",
    "        # Set up a single plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot each feature's curve for each cluster\n",
    "        for cluster_id, row in cluster_means.iterrows():\n",
    "            # Plot each feature's value for a specific cluster, linking the points with a line\n",
    "            plt.plot(features, row[features], label=f'Cluster {cluster_id}', marker='o', \n",
    "                     color=cluster_colors[cluster_id % len(cluster_colors)], linestyle='-', markersize=8)\n",
    "\n",
    "        # Customize plot\n",
    "        plt.title(f\"{title_prefix} ({n_clusters} Clusters)\", fontsize=16)\n",
    "        plt.xlabel('Distance (meters)', fontsize=14)\n",
    "        plt.ylabel('Population Values log (raw)', fontsize=14)\n",
    "\n",
    "        # Rename x-axis labels to descriptive names\n",
    "        x_labels = [f'{distances[0]} meters', f'{distances[1]} meters', f'{distances[2]} meters', f'{distances[3]} meters']\n",
    "        plt.xticks(ticks=np.arange(len(features)), labels=x_labels, rotation=45)\n",
    "\n",
    "        # Set y-axis ticks and labels\n",
    "        plt.yticks(ticks=y_ticks, labels=y_labels)\n",
    "\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1), title='Cluster')\n",
    "        plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "        plt.show()\n",
    "\n",
    "# Generate plots for multiple cluster solutions\n",
    "plot_feature_curves_multiple(data_selected, title_prefix=\"Feature Curves\", n_clusters_vector=n_clusters_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba0601-a2e9-4d77-bd0e-d2938028e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. CURVES ANALYSIS : SLOPES\n",
    "\n",
    "# Define the features to be used in the line plot\n",
    "features = [f'log_cc_Pop_estimation_sum_{distances[1]}_nw', 'slope_1_2', 'slope_2_3', 'slope_3_4']\n",
    "num_vars = len(features)\n",
    "\n",
    "# Create a scaled copy of the dataset for the line plots\n",
    "data_selected = nodes_selected.copy()\n",
    "\n",
    "# Define a list of colors for up to 20 clusters\n",
    "cluster_colors = [\n",
    "    'grey', 'lightgreen', 'darkgreen', 'lightblue', 'yellow', \n",
    "    'orange', 'mediumblue', 'darkblue', 'red', 'brown',\n",
    "    'purple', 'pink', 'cyan', 'olive', 'teal', \n",
    "    'gold', 'navy', 'lime', 'maroon', 'turquoise'\n",
    "]\n",
    "\n",
    "# Define a function to plot the feature curves for multiple cluster solutions\n",
    "def plot_feature_curves_multiple(data, title_prefix, n_clusters_vector):\n",
    "    for n_clusters in n_clusters_vector:\n",
    "        cluster_column = f'hckmeans_{n_clusters}_clusters'\n",
    "\n",
    "        # Compute the average of each feature per cluster in the specified column\n",
    "        cluster_means = data.groupby(cluster_column)[features].mean()\n",
    "\n",
    "        # Set up a single plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot each feature's curve for each cluster\n",
    "        for cluster_id, row in cluster_means.iterrows():\n",
    "            # Plot each feature's value for a specific cluster, linking the points with a line\n",
    "            plt.plot(features, row[features], label=f'Cluster {cluster_id}', marker='o', \n",
    "                     color=cluster_colors[cluster_id % len(cluster_colors)], linestyle='-', markersize=8)\n",
    "\n",
    "        # Customize plot\n",
    "        plt.title(f\"{title_prefix} ({n_clusters} Clusters)\", fontsize=16)\n",
    "        plt.xlabel('', fontsize=14)\n",
    "        plt.ylabel('Values', fontsize=14)\n",
    "\n",
    "        # Rename x-axis labels to more descriptive names\n",
    "        x_labels = ['pop_400(log)', 'slope 1-2', 'slope 2-3', 'slope 3-4']\n",
    "        plt.xticks(ticks=np.arange(len(features)), labels=x_labels, rotation=45)\n",
    "\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1), title='Cluster')\n",
    "        plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "        plt.show()\n",
    "\n",
    "# Generate plots for multiple cluster solutions\n",
    "plot_feature_curves_multiple(data_selected, title_prefix=\"Feature Curves\", n_clusters_vector=n_clusters_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f18ab-fc11-448e-ab80-836a3a7f14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################\n",
    "\n",
    "## APPENDICES\n",
    "\n",
    "# A1. Save Outputs\n",
    "gpkg = f'PPCA_A1_{Name}.gpkg'\n",
    "nodes_selected.to_file(gpkg, layer='Points_with_reg_and_hckmeans', driver=\"GPKG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
