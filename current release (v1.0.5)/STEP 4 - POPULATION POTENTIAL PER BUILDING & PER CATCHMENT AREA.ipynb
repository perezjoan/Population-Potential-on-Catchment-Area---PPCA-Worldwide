{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4a276-b684-4127-841a-c46637e14623",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "PPCA v1.0.4\n",
    "\n",
    "STEP 4: POPULATION POTENTIAL PER BUILDING & PER CATCHMENT AREA\n",
    "\n",
    "Author: Perez, Joan\n",
    "\n",
    "This script estimates population distribution within residential and mixed-use buildings based on floor area. It conducts a spatial \n",
    "join between building centroids and GHS population data, then disaggregates population values based on floor-area ratios. The population \n",
    "estimation is integrated into a pedestrian street network analysis, generating points along streets and associating population potential \n",
    "within various catchment areas. The output includes population estimations for buildings and pedestrian street network statistics across \n",
    "different catchment areas.\n",
    "\n",
    "Full description, metadata and output descriptions available here :\n",
    "https://github.com/perezjoan/Population-Potential-on-Catchment-Area---PPCA-Worldwide/tree/main\n",
    "\n",
    "Acknowledgement\n",
    "This resource was produced within the emc2 project, which is funded by ANR (France), FFG (Austria), MUR (Italy) and Vinnova (Sweden) under\n",
    "the Driving Urban Transition Partnership, which has been co-funded by the European Commission.\n",
    "\n",
    "License: Attribution-ShareAlike 4.0 International - CC-BY-SA-4.0 license\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1344b5-6b00-4415-be58-3a00195056af",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################\n",
    "\n",
    "# 0.1 : Box to fil with informations\n",
    "\n",
    "# Name of the case study. Expected format : Name = 'Nice'\n",
    "Name = \n",
    "\n",
    "# Recall the GHS date. Expected format : ghs_date = 2020  \n",
    "ghs_date = \n",
    "\n",
    "# Distance in meters between each point created along streets (catchment areas are computed on those points) \n",
    "# Expected format : points_distance = 20\n",
    "points_distance = \n",
    "\n",
    "# Distances for catchement areas (in meters). Expected format : distances = [160, 400, 800, 1200]\n",
    "distances = \n",
    "\n",
    "###########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451b113-eb53-4275-a5c7-5212346d74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2 : libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import momepy\n",
    "from cityseer.tools import mock, graphs, plot, io\n",
    "from cityseer.metrics import networks, layers\n",
    "from cityseer.tools.graphs import nx_consolidate_nodes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "from qgis.core import QgsApplication, QgsProcessingFeedback\n",
    "from qgis.analysis import QgsNativeAlgorithms\n",
    "import processing\n",
    "import sys\n",
    "from shapely.geometry import LineString, MultiLineString, Point\n",
    "from shapely import wkb\n",
    "from shapely.ops import split\n",
    "import fiona\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# 0.3 Data preparation\n",
    "print(\"Step 0.3: Loading data\")\n",
    "gpkg = f'PPCA_1-2_{Name}_retained.gpkg'\n",
    "population = gpd.read_file(gpkg, layer=f'ghs_populated_{ghs_date}_vector')\n",
    "gdf_lines = gpd.read_file(gpkg, layer=\"pedestrian_streets\")\n",
    "gpkg = f'PPCA_3-1_{Name}_IND_FL.gpkg'\n",
    "building = gpd.read_file(gpkg, layer='osm_buildings_FL_filled')\n",
    "gpkg_bis = f'PPCA_1-2_{Name}_retained.gpkg'\n",
    "gpkg_ter = f'PPCA_4-1_{Name}_POP_CAT.gpkg'\n",
    "\n",
    "## 1. POPULATION TO BUILDINGS\n",
    "\n",
    "# 1.1 JOIN GHS POPULATION TO BUILDINGS\n",
    "print(\"Step 1.1: Joining population data to buildings\")\n",
    "# Ensure population GeoDataFrame has the same CRS as building GeoDataFrame\n",
    "population = population.to_crs(building.crs)\n",
    "\n",
    "# Keep only residential buildings\n",
    "building = building[building['type_filled'] == 1]\n",
    "\n",
    "# Create a copy of building and calculate centroids\n",
    "building_centroids = building.copy()\n",
    "building_centroids['geometry'] = building_centroids.geometry.centroid\n",
    "\n",
    "# Drop existing columns named 'index_left' and 'index_right' if they exist\n",
    "building_centroids = building_centroids.drop(columns=['index_left', 'index_right'], errors='ignore')\n",
    "population = population.drop(columns=['index_left', 'index_right'], errors='ignore')\n",
    "\n",
    "# Perform the spatial join with centroids\n",
    "joined_data = gpd.sjoin(building_centroids, population, how='left', predicate='intersects', lsuffix='left', rsuffix='right')\n",
    "\n",
    "# 1.2 DISAGGREGATE GHS VALUES BASED ON FLOOR-AREA RATIO\n",
    "print(\"Step 1.2: Disaggregating GHS values based on floor-area ratio\")\n",
    "# Calculate the total FA sum for each index_right\n",
    "fa_sum = joined_data.groupby('index_right')['FA'].sum().rename('FA_sum').reset_index()\n",
    "\n",
    "# Merge the FA_sum back to the joined_data\n",
    "joined_data = joined_data.merge(fa_sum, on='index_right')\n",
    "\n",
    "# Calculate the FA ratio\n",
    "joined_data['FA_ratio'] = joined_data['FA'] / joined_data['FA_sum']\n",
    "\n",
    "# Disaggregate VALUE relative to the FA ratio\n",
    "joined_data['Pop_estimation'] = joined_data['VALUE'] * joined_data['FA_ratio']\n",
    "\n",
    "## 2. POPULATION TO CATCHMENT AREAS\n",
    "\n",
    "# 2.1 POINTS ALONG PEDESTRIAN STREET NETWORK\n",
    "print(\"Step 2.1: Generating points along the pedestrian street network\")\n",
    "# Offset distance : distance from each street crossing to set the first point (in meters)\n",
    "offset_distance = points_distance - 0.1\n",
    "\n",
    "# Function to generate points along a line at given intervals with an offset\n",
    "def generate_points(line, interval=points_distance, offset=offset_distance):\n",
    "    points = []\n",
    "    distance = offset\n",
    "    while distance < line.length:\n",
    "        point = line.interpolate(distance)\n",
    "        points.append(point)\n",
    "        distance += interval\n",
    "    return points\n",
    "\n",
    "# Create an empty GeoDataFrame to store the generated points\n",
    "gdf_points = gpd.GeoDataFrame(columns=['geometry'], crs=gdf_lines.crs)\n",
    "\n",
    "# Generate points along each street\n",
    "all_points = []\n",
    "for idx, row in gdf_lines.iterrows():\n",
    "    line = row.geometry\n",
    "    if isinstance(line, LineString):\n",
    "        points = generate_points(line, interval=points_distance, offset=offset_distance)\n",
    "        all_points.extend(points)\n",
    "    elif isinstance(line, MultiLineString):\n",
    "        for subline in line:\n",
    "            points = generate_points(subline, interval=points_distance, offset=offset_distance)\n",
    "            all_points.extend(points)\n",
    "\n",
    "# Create a GeoDataFrame from the generated points\n",
    "gdf_points = gpd.GeoDataFrame(geometry=all_points, crs=gdf_lines.crs)\n",
    "\n",
    "# 2.2 SPLIT STREETS ACCORDING TO POINT LOCATIONS\n",
    "print(\"Step 2.2: Splitting streets according to point locations\")\n",
    "# Create shifted points\n",
    "gdf_point_1 = gdf_points.copy()\n",
    "gdf_point_2 = gdf_points.copy()\n",
    "\n",
    "gdf_point_1.geometry = gdf_point_1.geometry.apply(lambda geom: Point(geom.x - 0.1, geom.y))\n",
    "gdf_point_2.geometry = gdf_point_2.geometry.apply(lambda geom: Point(geom.x + 0.1, geom.y))\n",
    "\n",
    "# Create connecting lines \n",
    "lines = []\n",
    "for point1, point2 in zip(gdf_point_1.geometry, gdf_point_2.geometry):\n",
    "    line = LineString([point1, point2])\n",
    "    lines.append(line)\n",
    "\n",
    "# Create a GeoDataFrame from the lines\n",
    "gdf_lines_between_points = gpd.GeoDataFrame(geometry=lines, crs=gdf_points.crs)\n",
    "gdf_lines_between_points.to_file(gpkg_ter, layer=\"points_catchment_stats\", driver=\"GPKG\")\n",
    "\n",
    "# Initialize QGIS Application\n",
    "qgis_path = \"C:/Program Files/QGIS 3.x/apps/qgis\"\n",
    "sys.path.append(qgis_path)\n",
    "QgsApplication.setPrefixPath(qgis_path, True)\n",
    "qgs = QgsApplication([], False)\n",
    "qgs.initQgis()\n",
    "\n",
    "# Add processing algorithms to registry\n",
    "from processing.core.Processing import Processing\n",
    "Processing.initialize()\n",
    "\n",
    "# Split lines\n",
    "print(\"Running QGIS 'splitwithlines' algorithm.\")\n",
    "splited_layer = \"splited_street\"\n",
    "processing.run(\"native:splitwithlines\",\n",
    "               {'INPUT':f\"{gpkg_bis}|layername=pedestrian_streets\",\n",
    "                'LINES':f\"{gpkg_ter}|layername=points_catchment_stats\",\n",
    "                'OUTPUT':f'ogr:dbname=\\'{gpkg_ter}\\' table=\"{splited_layer}\" (geom)'})\n",
    "print(\"Process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3405646-5c4b-4955-8575-1ffd29bd09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 REMOVE ZERO LENGTH EDGES\n",
    "\n",
    "gdf_lines = gpd.read_file(gpkg_ter, layer=\"splited_street\")\n",
    "\n",
    "# Multilines to singlelines\n",
    "gdf_lines = gdf_lines.explode(index_parts=True)\n",
    "\n",
    "# Converts a generic LineString gpd.GeoDataFrame to a cityseer compatible networkX MultiGraph\n",
    "nx_momepy = io.nx_from_generic_geopandas(gdf_lines)\n",
    "\n",
    "def remove_zero_length_edges(graph):\n",
    "    edges_to_remove = []\n",
    "    \n",
    "    # Iterate over edges in the graph\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        # Ensure the edge data has a geometry\n",
    "        if 'geometry' in data:\n",
    "            line = data['geometry']\n",
    "            \n",
    "            # Calculate the length of the edge\n",
    "            if isinstance(line, LineString):\n",
    "                length = line.length\n",
    "            else:\n",
    "                length = 0\n",
    "\n",
    "            # Check if the length is zero\n",
    "            if length == 0:\n",
    "                edges_to_remove.append((u, v))\n",
    "    \n",
    "    # Remove zero-length edges from the graph\n",
    "    graph.remove_edges_from(edges_to_remove)\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Remove zero length edges\n",
    "nx_momepy = remove_zero_length_edges(nx_momepy)\n",
    "\n",
    "# 2.4 CONSOLIDATE AND COMPUTE STATS ALONG GRAPH\n",
    "\n",
    "# Remove nodes within {point_distance} meters of each other\n",
    "nx_momepy = nx_consolidate_nodes(\n",
    "    nx_multigraph=nx_momepy,\n",
    "    buffer_dist=(points_distance / 2) - 0.1,\n",
    "    neighbour_policy=\"direct\",\n",
    "    crawl=True,\n",
    "    centroid_by_itx=True,\n",
    "    prioritise_by_hwy_tag=False,\n",
    "    merge_edges_by_midline=False,\n",
    "    contains_buffer_dist=1,\n",
    "    osm_hwy_target_tags=None,\n",
    "    osm_matched_tags_only=False\n",
    ")\n",
    "\n",
    "nodes_gdf, edges_gdf, network_structure = io.network_structure_from_nx(\n",
    "    nx_momepy, crs=gdf_lines.crs.to_epsg()\n",
    ")\n",
    "\n",
    "population = gpd.GeoDataFrame(joined_data, geometry='geometry')\n",
    "population.index = population.index.astype(str)\n",
    "\n",
    "# Assign GeoDataFrame to network with an appropriate max_netw_assign_dist\n",
    "data_map, assigned_population_gdf = layers.assign_gdf_to_network(\n",
    "    data_gdf=population,\n",
    "    network_structure=network_structure,\n",
    "    max_netw_assign_dist=160,\n",
    ")\n",
    "\n",
    "# compute_stats function\n",
    "nodes_gdf, numerical_gdf = layers.compute_stats(\n",
    "    data_gdf=assigned_population_gdf,\n",
    "    stats_column_label=\"Pop_estimation\",\n",
    "    nodes_gdf=nodes_gdf,\n",
    "    network_structure=network_structure,\n",
    "    max_netw_assign_dist=160,\n",
    "    distances=distances,\n",
    ")\n",
    "\n",
    "# Drop all columns that end with '_wt'\n",
    "columns_to_drop = [col for col in nodes_gdf.columns if col.endswith('_wt')]\n",
    "nodes_gdf = nodes_gdf.drop(columns=columns_to_drop)\n",
    "\n",
    "# 2.5 COMPUTE STATS ALONG PEDESTRIAN STREETS\n",
    "\n",
    "gpkg = f'PPCA_1-2_{Name}_retained.gpkg'\n",
    "gdf_lines = gpd.read_file(gpkg, layer=\"pedestrian_streets\")\n",
    "\n",
    "# Create a unique ID column 'NUMID'\n",
    "gdf_lines['NUMID'] = range(1, len(gdf_lines) + 1)\n",
    "\n",
    "# Create a 1m buffer around the lines\n",
    "buffered_lines = gdf_lines.copy()\n",
    "buffered_lines['geometry'] = buffered_lines.geometry.buffer(1)  # 1m buffer\n",
    "\n",
    "# Perform the nearest spatial join\n",
    "nodes_with_numid = gpd.sjoin_nearest(nodes_gdf, buffered_lines[['NUMID', 'geometry']], how='left', distance_col='distance')\n",
    "\n",
    "# Drop the 'distance' column\n",
    "nodes_with_numid = nodes_with_numid.drop(columns=['distance'])\n",
    "\n",
    "# Fill any missing values with 0 or appropriate values before aggregation\n",
    "nodes_with_numid.fillna(0, inplace=True)\n",
    "\n",
    "# Create an aggregation dictionary only for average population estimates based on distances\n",
    "agg_dict = {}\n",
    "\n",
    "for dist in distances:\n",
    "    agg_dict[f'avg_pop_{dist}'] = (f'cc_Pop_estimation_sum_{dist}_nw', 'mean')\n",
    "\n",
    "# Perform the groupby operation with the dynamically created aggregation dictionary\n",
    "grouped_stats = nodes_with_numid.groupby('NUMID').agg(**agg_dict).reset_index()\n",
    "\n",
    "# Ensure each NUMID is unique by merging the results back with gdf_lines\n",
    "gdf = gdf_lines.merge(grouped_stats, on='NUMID', how='left')\n",
    "print(\"Process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d09b31-b6fa-4d88-bd8a-62381dc619af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 FILL NULL VALUES FOR SMALL SEGMENTS LACKING POINTS (AVG OF INTERSECTING SEGMENTS) \n",
    "# Optional, heavy computational load\n",
    "\n",
    "# Function to perform the filling process\n",
    "def fill_null_values(gdf, distances, buffer_distance):\n",
    "    # Step 1: Create a buffer around segments with NULL values\n",
    "    gdf['buffer'] = gdf['geometry'].buffer(buffer_distance)\n",
    "\n",
    "    # Step 2: Loop over each distance and perform the fill\n",
    "    for dist in distances:\n",
    "        col_name = f'avg_pop_{dist}'\n",
    "        \n",
    "        # Create a separate GeoDataFrame for each column\n",
    "        gdf_with_value = gdf[~gdf[col_name].isnull()].copy()\n",
    "        gdf_without_value = gdf[gdf[col_name].isnull()].copy()\n",
    "\n",
    "        # Step 3: Spatial join to find nearby segments\n",
    "        # We'll use an intersection join between the buffered segments and the ones with values\n",
    "        for idx, row in gdf_without_value.iterrows():\n",
    "            # Create a small buffer for the current row\n",
    "            buffer = row['geometry'].buffer(buffer_distance)\n",
    "\n",
    "            # Find nearby segments by intersection\n",
    "            nearby_segments = gpd.sjoin(gdf_with_value, gpd.GeoDataFrame(geometry=[buffer], crs=gdf.crs), how='inner', predicate='intersects')\n",
    "\n",
    "            # Step 4: Calculate the average value of the nearby segments\n",
    "            if len(nearby_segments) > 0:\n",
    "                gdf_without_value.loc[idx, col_name] = nearby_segments[col_name].mean()\n",
    "\n",
    "        # Step 5: Update the main GeoDataFrame with the filled values for the current distance\n",
    "        gdf.loc[gdf[col_name].isnull(), col_name] = gdf_without_value[col_name]\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Step 1: Run the fill process the first time\n",
    "buffer_distance = 1  # Adjust as needed\n",
    "gdf_filled = fill_null_values(gdf, distances, buffer_distance)\n",
    "\n",
    "# Step 2: Check if there are still NULL values, and repeat the process\n",
    "gdf_filled = fill_null_values(gdf_filled, distances, buffer_distance)\n",
    "\n",
    "# Drop the buffer column if not needed\n",
    "gdf_filled.drop(columns=['buffer'], inplace=True)\n",
    "\n",
    "# Assign the result back to the original GeoDataFrame\n",
    "gdf = gdf_filled\n",
    "print(\"Process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24ab63-ce23-4034-a893-c2922ce62ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################\n",
    "\n",
    "## APPENDICES\n",
    "\n",
    "# A1. Save Outputs\n",
    "joined_data.to_file(gpkg_ter, layer='osm_buildings_pop_estimate', driver=\"GPKG\")\n",
    "nodes_gdf.to_file(gpkg_ter, layer='points_catchment_stats', driver=\"GPKG\")\n",
    "gdf.to_file(gpkg_ter, layer='pedestrian_streets_avg_pop', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8af4a-cdb5-4323-a6f8-758f7a50a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2. Map of population\n",
    "# Size of the markers\n",
    "marker_size_scale = 0.01\n",
    "\n",
    "# Plotting the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot the base map - assuming population GeoDataFrame provides the base map context\n",
    "population.plot(ax=ax, color='lightgrey', edgecolor='white', alpha=0.5)\n",
    "\n",
    "# Plot the centroids with VALUE_disaggregated\n",
    "joined_data.plot(ax=ax, color='red', markersize=joined_data['Pop_estimation'] * marker_size_scale, alpha=0.2)\n",
    "\n",
    "# Normalize the data for color mapping\n",
    "#norm = colors.Normalize(vmin=nodes_gdf[column_name].min(), vmax=nodes_gdf[column_name].max())\n",
    "cmap = cm.viridis_r  # Use the reversed colormap\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Disaggregated population by residential building', fontsize=15)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf685e1-992e-47f7-83a1-f24676a0ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3. Nodes vs building centroids with population\n",
    "# Visualize the network and the population points\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "nodes_gdf.plot(ax=ax, color='blue', markersize=1, label='Network Nodes')\n",
    "population.plot(ax=ax, color='red', markersize=1, label='Population Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be653b9a-c332-4d47-b955-2a45d0c52a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm, colors\n",
    "\n",
    "# Define the bins\n",
    "bins = [0, 100, 500, 1000, 5000, 10000, 20000]  # Adjust bins as needed\n",
    "\n",
    "# Loop over each distance value\n",
    "for i, distance_value in enumerate(distances):\n",
    "    \n",
    "    # Generate the column name dynamically\n",
    "    column_name = f'cc_Pop_estimation_sum_{distance_value}_nw'\n",
    "\n",
    "    # Size of the markers\n",
    "    marker_size_scale = 0.01\n",
    "\n",
    "    # Plotting the map\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "\n",
    "    # Create a colormap and set the normalization using bins\n",
    "    cmap = cm.viridis_r  # Use the reversed colormap\n",
    "    norm = colors.BoundaryNorm(bins, ncolors=cmap.N, clip=True)\n",
    "\n",
    "    # Plot the centroids with color mapping based on the dynamically generated column name\n",
    "    nodes_gdf.plot(\n",
    "        ax=ax,\n",
    "        column=column_name,\n",
    "        cmap=cmap,\n",
    "        markersize=marker_size_scale,\n",
    "        alpha=0.6,\n",
    "        legend=False,\n",
    "        norm=norm  # Apply the normalization\n",
    "    )\n",
    "\n",
    "    # Adding titles and labels\n",
    "    plt.title(f'Population Potential per {points_distance}m node for catchment area of {distance_value}m', fontsize=15)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Adding color bar\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax)\n",
    "    cbar.set_label(f'Population Estimation Sum ({distance_value})', fontsize=12)\n",
    "    cbar.set_ticks(bins)  # Set ticks to match bin edges\n",
    "    cbar.set_ticklabels([f'{b:,}' if b != 100000 else 'max' for b in bins])  # Replace 100000 with 'max'\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61253e97-92a0-4c62-a657-463b131d57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Loop over each distance value\n",
    "for i, distance_value in enumerate(distances):\n",
    "    \n",
    "    # Plotting the map\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Plot the base map - assuming gdf provides the base map context\n",
    "    gdf.plot(ax=ax, color='lightgrey', edgecolor='white', alpha=0.5)\n",
    "    \n",
    "    # Set the population bins for classification\n",
    "    bins = [0, 600, 1200, 2400, 4800, 100000]\n",
    "    \n",
    "    # Create a colormap using the reversed magma colormap\n",
    "    cmap = cm.magma_r\n",
    "    norm = colors.BoundaryNorm(bins, ncolors=cmap.N, clip=True)\n",
    "    \n",
    "    # Classify the data into bins and apply the reversed magma colormap\n",
    "    gdf['pop_bins'] = pd.cut(gdf[f'avg_pop_{distance_value}'], bins=bins, labels=False, include_lowest=True)\n",
    "    \n",
    "    # Plot the lines with width proportional to population and color based on bins\n",
    "    gdf.plot(\n",
    "        ax=ax,\n",
    "        column='pop_bins',  # Use the classified population bins for color\n",
    "        cmap=cmap,\n",
    "        linewidth=2,  # Adjust as needed\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Add a colorbar on the right side using the corrected color map and bins\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    sm.set_array([])  # Empty array for the colorbar\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "    cbar.set_label('Population per Segment')\n",
    "    \n",
    "    # Update tick labels to replace '100000' with 'max'\n",
    "    cbar.set_ticks(bins)  # Set ticks to match bin edges\n",
    "    cbar.set_ticklabels(['0', '600', '1200', '2400', '4800', 'max'])\n",
    "    \n",
    "    # Adding titles and labels\n",
    "    plt.title(f'Population Average per Segment for a Catchment Area of {distance_value}m', fontsize=15)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
